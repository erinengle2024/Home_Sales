# Home_Sales - Big Data Challenge


Welcome to my submission for the Big Data Challenge in module 22 of the Data Analytics bootcamp. This project was completed as part of the curriculum to showcase my proficiency with Big Data libraries such as PySpark, and the parquet date file format. 


# Challenge Description
In this challenge, we used SparkSQL to analyze home sales data by determining key metrics, such as the average price of homes based on various criteria like the number of bedrooms, bathrooms, and other features. We optimized query performance by caching tables and partitioning data, then compared runtime metrics to assess efficiency improvements. The project demonstrated the practical application of SparkSQL for big data processing and performance optimization.
![image](https://github.com/user-attachments/assets/b1198a12-79f6-47ff-af26-f336faa7b6db)
![image](https://github.com/user-attachments/assets/8c67ed8b-b10a-402f-9a35-8f8c3524f9da)



# Tools Used

![image](https://github.com/erinengle2024/deep-learning-challenge/assets/158017994/68888965-6df2-441c-b274-cfcba0c3e370)
![image](https://github.com/erinengle2024/web-scraping-challenge/assets/158017994/afb2a124-27eb-4ddb-ad3a-2694b645c7f1)
![image](https://github.com/erinengle2024/web-scraping-challenge/assets/158017994/51f91ce4-e15e-4707-969b-81a9bbf1f83c)
![image](https://github.com/user-attachments/assets/437014e3-0bbb-4f18-9e35-53cb9679477d)
![image](https://github.com/erinengle2024/deep-learning-challenge/assets/158017994/d1f6f2d2-4f2f-4935-8ca8-41195c27fe70)







  
  # My Results
![image](https://github.com/user-attachments/assets/ac31c1a8-b5c8-452a-8b08-6f41aadfe554)
![image](https://github.com/user-attachments/assets/2a4459c6-f999-47ad-b78b-2144d0059e49)
![image](https://github.com/user-attachments/assets/a23beba5-152d-47b9-ab2f-f3e831a11204)
![image](https://github.com/user-attachments/assets/ac119521-c4ca-4e39-871f-b934978bf7ac)
![image](https://github.com/user-attachments/assets/0d110048-f694-4ad3-8bb0-c84f8c81898f)
![image](https://github.com/user-attachments/assets/4869854d-c536-4f69-86e8-ff35298d802a)
![image](https://github.com/user-attachments/assets/0afd9797-db9a-4009-a2c2-2b3812aa600c)
![image](https://github.com/user-attachments/assets/ad7a886c-d164-441f-9a95-454e44159b0c)















# Resources Used

Challenges I drew heavily from include the following:
 - The Spark Filtering Activity
 - The Spark Dates Activity
 - The Temporary Views Activity
 - The Read and Write Parquet Activity
 - The Parquet Partition Activity
 - The Cache Broadcast Activity





These challenges, in addition to the other challenges provided, helped teach us the following:
- How to use pyspark commands to filter and aggregate
- How to create temporary views
- How to utilize the parquet file format
- How to start and stop a cache session

 






 ## Additional Resources
For syntax that I was not familiar with, I used resources such as the Xpert Learning Assistant, provided by UC Berkely Bootcamp, PySpark Documentation, and ChatGPT.  
